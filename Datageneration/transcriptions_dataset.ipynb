{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd04cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462",
   "display_name": "Python 3.9.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Here the manually transcribed data is formatted to the same format as the generated data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Functions wich handles the formatting portion of the read."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def retrieve_entities(sentence):\n",
    "    # Filter the sentence structure from the file\n",
    "    return list(filter(lambda x: len(x)  > 2 and \"[\" not in x and \"]\" not in x, sentence.split('\"')))\n",
    "\n",
    "def add_entities(ents, tag):\n",
    "    # Add all incoming entities and add a correct tag to them.\n",
    "    res = []\n",
    "    for ent in ents:\n",
    "        res.append([ent.lower(), tag])\n",
    "    return res\n",
    "\n",
    "def finalize_dialogue(moving, ipl, gpe, topics, sentences):\n",
    "    sentences = sentences.lower()\n",
    "    # Format the entities on to the correct form as the generated data. \n",
    "    entities = add_entities(moving, \"COM\") + add_entities(ipl, \"IPL\") + add_entities(gpe, \"GPE\")\n",
    "    entity_labels = gen.get_labels(sentences, entities)\n",
    "\n",
    "    topic_array = [False for i in range(len(gen.topic_categories))]\n",
    "    topic_labels = dict(zip(gen.topic_categories, topic_array))\n",
    "    for topic in topics:\n",
    "        if topic_labels.get(topic, None) != None:\n",
    "            topic_labels[topic] = True\n",
    "\n",
    "    save_format = {\n",
    "        \"dialogue\": sentences,\n",
    "        \"entities\": entity_labels,\n",
    "        \"cats\": [[key, topic_labels[key]] for key in topic_labels]\n",
    "    }\n",
    "\n",
    "    return json.dumps(save_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from modules.generator import Generator\n",
    "gen = Generator(None, None)\n",
    "res = []\n",
    "with open(\"transcriptions.txt\", \"r\") as f:\n",
    "    moving_entities = []\n",
    "    ipl = []\n",
    "    gpe = []\n",
    "    topics = []\n",
    "    sentences = \"\"\n",
    "\n",
    "    for line in f:\n",
    "        split = line.split(\":\")\n",
    "\n",
    "        if len(split) > 1:\n",
    "            # Split into sentence and category\n",
    "            cat, sentence = split\n",
    "        else:\n",
    "            # End of a dialogue\n",
    "            dialogue = finalize_dialogue(moving_entities, ipl, gpe, topics, sentences)\n",
    "            res.append(dialogue)\n",
    "\n",
    "            # Reset the entities\n",
    "            moving_entities = []\n",
    "            ipl = []\n",
    "            gpe = []\n",
    "            topics = []\n",
    "            sentences = \"\"\n",
    "            continue\n",
    "        \n",
    "        # Add the correct element to correct entity or sentence structure\n",
    "        if cat == \"ENTITY\":\n",
    "            moving_entities += retrieve_entities(sentence)\n",
    "        elif cat == \"IPL\":\n",
    "            ipl += retrieve_entities(sentence)\n",
    "        elif cat == \"GPE\":\n",
    "            gpe += retrieve_entities(sentence)\n",
    "        elif cat == \"TOPIC\":\n",
    "            topics += retrieve_entities(sentence)\n",
    "        else:\n",
    "            sentences += \" \".join(sentence.rsplit()) + \" \""
   ]
  },
  {
   "source": [
    "Saving the dialogues to a set file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"{\\\"dialogue\\\": \\\"vts oxel\\\\u00f6sund svart\\\\u00f6. svarte vts tv\\\\u00e5 tusen. anchorage away close of vinterklasen outbound for sea no pilot required. svarte anchor away proceeding outbound for sea. you have outbound vessle godafors approaching vinterklasen and inbound vessle stillmover at kr\\\\u00e4nkan s\\\\u00e4nd\\\\u00f6hook (sandihook) in korph\\\\u00e5let. it's okey well understood thank you. then you have inbound vessle white force queen inbound kr\\\\u00e4nkan. white force queen inbound approaching kr\\\\u00e4nkan. \\\", \\\"entities\\\": [[\\\"godafors\\\", 190, 198, \\\"COM\\\"], [\\\"stillmover\\\", 243, 253, \\\"COM\\\"], [\\\"svarte\\\", 22, 28, \\\"COM\\\"], [\\\"svarte\\\", 117, 123, \\\"COM\\\"], [\\\"svart\\\\u00f6\\\", 14, 20, \\\"COM\\\"], [\\\"vinterklasen\\\", 68, 80, \\\"IPL\\\"], [\\\"vinterklasen\\\", 211, 223, \\\"IPL\\\"], [\\\"vts oxel\\\\u00f6sund\\\", 0, 13, \\\"COM\\\"], [\\\"vts tv\\\\u00e5 tusen\\\", 29, 42, \\\"COM\\\"], [\\\"white force queen\\\", 367, 384, \\\"COM\\\"], [\\\"white force queen\\\", 402, 419, \\\"COM\\\"]], \\\"cats\\\": [[\\\"traffic information\\\", false], [\\\"inbound\\\", false], [\\\"outbound\\\", true], [\\\"pilot required\\\", false]]}\"\n"
     ]
    }
   ],
   "source": [
    "with open(\"../Dataset/transcribed_dialogues.json\", \"w\") as f:\n",
    "    print(json.dumps(res[0]))\n",
    "    f.write(\"\\n\".join(res))"
   ]
  },
  {
   "source": [
    "Try loading the transcribed dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Transcribed sentences:  37\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "with open(\"../Dataset/transcribed_dialogues.json\", \"r\") as f:\n",
    "    for ind, line in enumerate(f):\n",
    "        json_line = json.loads(line)\n",
    "        data.append(json_line)\n",
    "\n",
    "print(\"Transcribed sentences: \", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}